# Some cool playlist urls:
# - Re:Invent Compute 2024: https://www.youtube.com/watch?v=u2jDFYvXgDE&list=PL2yQDdvlhXf9-pNfjMjlU8uk7C4qu-HI-
# - KubeCon North America 2024: https://www.youtube.com/playlist?list=PLj6h78yzYM2Pw4mRw4S-1p_xLARMqPkA7
# - GoogleNext 2024: https://www.youtube.com/playlist?list=PLIivdWyY5sqLHU6fh9ozZ7mxsj7GcPjRF
# - Microsoft Build 2024: https://www.youtube.com/playlist?list=PLlrxD0HtieHghp_QCcQ2JiTXY3qO9oPDu
# - DeVops Conf 2024: https://www.youtube.com/playlist?list=PLvo3403u1dyjmTf6mhCRV_ZuvzMXTpqZV
# - PyConf 2024 : https://www.youtube.com/playlist?list=PL2Uw4_HvXqvYhjub9bw4uDAmNtprgAvlJ


# YouTube Configuration
PLAYLIST_URL=
VIDEOS=                   # Optional: Number of videos to process (default: all)
CATEGORIES=              # Optional: Filter by categories (comma-separated)

# Processing Configuration
BATCH_SIZE=1            # Number of videos to process concurrently
MODEL=llama3.2          # Model to use (llama3.2, mistral, claude, claude-haiku, nova)
VERBOSE=false           # Show detailed progress information

# Hardware Configuration
NUM_GPUS=0             # Number of GPUs to use (for Ollama models)
NUM_CPUS=4             # Number of CPU cores to use
THREADS=4              # Number of CPU threads for LLM

# AWS Configuration (required for Bedrock models)

AWS_DEFAULT_REGION=    # Required for Bedrock models (e.g., us-east-1)
AWS_PROFILE=           # Required for Bedrock models

# Output Configuration
OUTPUT=                # Optional: Custom output file path 